{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    required = [\n",
    "        'punkt',\n",
    "        'punkt_tab',\n",
    "        'averaged_perceptron_tagger',\n",
    "        'averaged_perceptron_tagger_eng',\n",
    "        'words',\n",
    "        'wordnet',\n",
    "        'stopwords',\n",
    "        'omw-1.4',\n",
    "        'maxent_ne_chunker',\n",
    "        'maxent_ne_chunker_tab'\n",
    "    ]\n",
    "    for resource in required:\n",
    "        try:\n",
    "            nltk.data.find(f'{resource}')\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK resource: {resource}...\")\n",
    "            nltk.download(resource)\n",
    "ensure_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b2f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Natural language processing with TF-IDF is powerful. It extracts important keywords from text.\",\n",
    "    \"TF-IDF helps in identifying significant words. Keyword extraction using NLP techniques is common.\",\n",
    "    \"Apple Inc. is planning to open a new office in London. Tim Cook will attend the opening ceremony in July 2025.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4930612",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#stemmer = PorterStemmer() #lemmatizer is used\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlpNER = spacy.load(\"en_core_web_sm\")# for named entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9913524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# POS tag conversion for lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def preprocess_Seg_Token_POS(doc):\n",
    "    sentences = sent_tokenize(doc) #Segmentation\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().replace(\".\", \"\") \n",
    "        tokens = word_tokenize(sentence) #Tokenize\n",
    "        tagged = pos_tag(tokens) #POS_PARSING\n",
    "        all_words.append(tagged)\n",
    "    return all_words\n",
    "def preprocess_stop_words_lemmatizer(doc):\n",
    "    all_words = []\n",
    "    for tagged in doc:\n",
    "        for word, tag in tagged:\n",
    "            #Removing Stop Words\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                #STEMMING or lemmating \n",
    "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "                all_words.append(lemma)\n",
    "    return all_words\n",
    "def preprocess_doc(doc):\n",
    "    seg_Token_POS = preprocess_Seg_Token_POS(doc)\n",
    "    stop_words_lemmatizer = preprocess_stop_words_lemmatizer(seg_Token_POS)\n",
    "    return ' '.join(stop_words_lemmatizer)\n",
    "def preprocess(doc):\n",
    "    sentences = sent_tokenize(doc) #Segmentation\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().replace(\".\", \"\") \n",
    "        tokens = word_tokenize(sentence) #Tokenize\n",
    "        tagged = pos_tag(tokens) #POS_PARSING\n",
    "        for word, tag in tagged:\n",
    "            #Removing Stop Words\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                #STEMMING or lemmating \n",
    "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "                all_words.append(lemma)\n",
    "    return ' '.join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cad95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top keywords in Document 1:\n",
      "extract       0.353553\n",
      "language      0.353553\n",
      "important     0.353553\n",
      "keywords      0.353553\n",
      "processing    0.353553\n",
      "Name: Doc1, dtype: float64\n",
      "\n",
      "Top keywords in Document 2:\n",
      "common        0.316228\n",
      "extraction    0.316228\n",
      "help          0.316228\n",
      "identify      0.316228\n",
      "keyword       0.316228\n",
      "Name: Doc2, dtype: float64\n",
      "\n",
      "Top keywords in Document 3:\n",
      "apple       0.27735\n",
      "attend      0.27735\n",
      "ceremony    0.27735\n",
      "cook        0.27735\n",
      "inc         0.27735\n",
      "Name: Doc3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Preprocess documents\n",
    "#processed_docs = [preprocess(doc) for doc in docs]\n",
    "seg_Token_POSs = [preprocess_Seg_Token_POS(doc) for doc in docs]\n",
    "\n",
    "stop_words_lemmatizers = [preprocess_stop_words_lemmatizer(doc) for doc in seg_Token_POSs]\n",
    "processed_docs = [' '.join(doc) for doc in stop_words_lemmatizers]\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "df = pd.DataFrame(tfidf_matrix.T.toarray(), index=feature_names, columns=[f'Doc{i+1}' for i in range(len(docs))])\n",
    "posArr = np.empty(len(feature_names), dtype=f'<U{len(feature_names)}')\n",
    "for i in range(len(posArr)):\n",
    "    for poss in seg_Token_POSs:\n",
    "        for pos in poss:\n",
    "            for word, tag in pos:\n",
    "                if(word == feature_names[i]):\n",
    "                    posArr[i] = tag\n",
    "\n",
    "df['POS'] = posArr\n",
    "df.head()\n",
    "for i in range(len(docs)):\n",
    "    print(f\"\\nTop keywords in Document {i+1}:\")\n",
    "    top_keywords = df[f'Doc{i+1}'].sort_values(ascending=False).head(5)\n",
    "    print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2147c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë TF-IDF Keywords:\n",
      "\n",
      "Sentence 1:\n",
      "text: 0.3536\n",
      "processing: 0.3536\n",
      "powerful: 0.3536\n",
      "natural: 0.3536\n",
      "important: 0.3536\n",
      "keywords: 0.3536\n",
      "language: 0.3536\n",
      "extract: 0.3536\n",
      "\n",
      "Sentence 2:\n",
      "word: 0.3162\n",
      "use: 0.3162\n",
      "technique: 0.3162\n",
      "significant: 0.3162\n",
      "nlp: 0.3162\n",
      "identify: 0.3162\n",
      "keyword: 0.3162\n",
      "help: 0.3162\n",
      "extraction: 0.3162\n",
      "common: 0.3162\n",
      "\n",
      "Sentence 3:\n",
      "tim: 0.2774\n",
      "opening: 0.2774\n",
      "new: 0.2774\n",
      "office: 0.2774\n",
      "open: 0.2774\n",
      "plan: 0.2774\n",
      "london: 0.2774\n",
      "cook: 0.2774\n",
      "apple: 0.2774\n",
      "attend: 0.2774\n",
      "ceremony: 0.2774\n",
      "july: 0.2774\n",
      "inc: 0.2774\n"
     ]
    }
   ],
   "source": [
    "print(\"üîë TF-IDF Keywords:\")\n",
    "for i, sentence in enumerate(processed_docs):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    scores = tfidf_matrix[i].toarray()[0]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    for idx in sorted_indices[:]: \n",
    "        if scores[idx] > 0:\n",
    "            print(f\"{feature_names[idx]}: {scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15d0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑ Named Entities:\n",
      "Natural: ADJ\n",
      "language: NOUN\n",
      "processing: NOUN\n",
      "with: ADP\n",
      "TF: PROPN\n",
      "-: PUNCT\n",
      "IDF: PROPN\n",
      "is: AUX\n",
      "powerful: ADJ\n",
      ".: PUNCT\n",
      "It: PRON\n",
      "extracts: VERB\n",
      "important: ADJ\n",
      "keywords: NOUN\n",
      "from: ADP\n",
      "text: NOUN\n",
      ".: PUNCT\n",
      "TF: PROPN\n",
      "-: PUNCT\n",
      "IDF: PROPN\n",
      "helps: VERB\n",
      "in: ADP\n",
      "identifying: VERB\n",
      "significant: ADJ\n",
      "words: NOUN\n",
      ".: PUNCT\n",
      "Keyword: PROPN\n",
      "extraction: NOUN\n",
      "using: VERB\n",
      "NLP: PROPN\n",
      "techniques: NOUN\n",
      "is: AUX\n",
      "common: ADJ\n",
      ".: PUNCT\n",
      "Keyword: PERSON\n",
      "NLP: ORG\n",
      "Apple: PROPN\n",
      "Inc.: PROPN\n",
      "is: AUX\n",
      "planning: VERB\n",
      "to: PART\n",
      "open: VERB\n",
      "a: DET\n",
      "new: ADJ\n",
      "office: NOUN\n",
      "in: ADP\n",
      "London: PROPN\n",
      ".: PUNCT\n",
      "Tim: PROPN\n",
      "Cook: PROPN\n",
      "will: AUX\n",
      "attend: VERB\n",
      "the: DET\n",
      "opening: NOUN\n",
      "ceremony: NOUN\n",
      "in: ADP\n",
      "July: PROPN\n",
      "2025: NUM\n",
      ".: PUNCT\n",
      "Apple Inc.: ORG\n",
      "London: GPE\n",
      "Tim Cook: PERSON\n",
      "July 2025: DATE\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüè∑ Named Entities:\")\n",
    "named = [nlpNER(doc) for doc in docs]\n",
    "for elm in named:\n",
    "    for ent in elm:\n",
    "        print(f\"{ent.text}: {ent.pos_}\")\n",
    "    for ent in elm.ents:\n",
    "        print(f\"{ent.text}: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607a9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, messagebox\n",
    "import PyPDF2\n",
    "\n",
    "# Main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Multi PDF Reader\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "text_display = scrolledtext.ScrolledText(root, wrap=tk.WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242b5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pdfs():\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select PDF files\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\")]\n",
    "    )\n",
    "\n",
    "    if not file_paths:\n",
    "        return\n",
    "\n",
    "    text_display.delete(1.0, tk.END)  # Clear existing text\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                text_display.insert(tk.END, f\"\\n--- {file_path} ---\\n\\n\")\n",
    "                for page in reader.pages:\n",
    "                    text_display.insert(tk.END, page.extract_text())\n",
    "                    text_display.insert(tk.END, \"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Could not read {file_path}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ed0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Button to open PDFs\n",
    "open_button = tk.Button(root, text=\"Open PDF Files\", command=open_pdfs)\n",
    "open_button.pack(pady=10)\n",
    "\n",
    "# Scrollable text box\n",
    "text_display.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
