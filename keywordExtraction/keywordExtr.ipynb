{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b4b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    required = [\n",
    "        'punkt',\n",
    "        'punkt_tab',\n",
    "        'averaged_perceptron_tagger',\n",
    "        'averaged_perceptron_tagger_eng',\n",
    "        'words',\n",
    "        'wordnet',\n",
    "        'stopwords',\n",
    "        'omw-1.4',\n",
    "        'maxent_ne_chunker',\n",
    "        'maxent_ne_chunker_tab'\n",
    "    ]\n",
    "    for resource in required:\n",
    "        try:\n",
    "            nltk.data.find(f'{resource}')\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK resource: {resource}...\")\n",
    "            nltk.download(resource)\n",
    "ensure_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b2f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Natural language processing with TF-IDF is powerful. It extracts important keywords from text.\",\n",
    "    \"TF-IDF helps in identifying significant words. Keyword extraction using NLP techniques is common.\",\n",
    "    \"Apple Inc. is planning to open a new office in London. Tim Cook will attend the opening ceremony in July 2025.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4930612",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#stemmer = PorterStemmer() #lemmatizer is used\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlpNER = spacy.load(\"en_core_web_sm\")# for named entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9913524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# POS tag conversion for lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "def preprocess_Seg_Token_POS(doc):\n",
    "    sentences = sent_tokenize(doc) #Segmentation\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().replace(\".\", \"\") \n",
    "        tokens = word_tokenize(sentence) #Tokenize\n",
    "        tagged = pos_tag(tokens) #POS_PARSING\n",
    "        all_words.append(tagged)\n",
    "    return all_words\n",
    "def preprocess_stop_words_lemmatizer(doc):\n",
    "    all_words = []\n",
    "    for tagged in doc:\n",
    "        for word, tag in tagged:\n",
    "            #Removing Stop Words\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                #STEMMING or lemmating \n",
    "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "                all_words.append(lemma)\n",
    "    return all_words\n",
    "def preprocess_doc(doc):\n",
    "    seg_Token_POS = preprocess_Seg_Token_POS(doc)\n",
    "    stop_words_lemmatizer = preprocess_stop_words_lemmatizer(seg_Token_POS)\n",
    "    return ' '.join(stop_words_lemmatizer)\n",
    "def preprocess(doc):\n",
    "    sentences = sent_tokenize(doc) #Segmentation\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().replace(\".\", \"\") \n",
    "        tokens = word_tokenize(sentence) #Tokenize\n",
    "        tagged = pos_tag(tokens) #POS_PARSING\n",
    "        for word, tag in tagged:\n",
    "            #Removing Stop Words\n",
    "            if word.isalpha() and word not in stop_words:\n",
    "                #STEMMING or lemmating \n",
    "                lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "                all_words.append(lemma)\n",
    "    return ' '.join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cad95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc1</th>\n",
       "      <th>Doc2</th>\n",
       "      <th>Doc3</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attend</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ceremony</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cook</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc1      Doc2     Doc3 POS\n",
       "apple      0.0  0.000000  0.27735  NN\n",
       "attend     0.0  0.000000  0.27735  VB\n",
       "ceremony   0.0  0.000000  0.27735  NN\n",
       "common     0.0  0.316228  0.00000  JJ\n",
       "cook       0.0  0.000000  0.27735  NN"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess documents\n",
    "#processed_docs = [preprocess(doc) for doc in docs]\n",
    "seg_Token_POSs = [preprocess_Seg_Token_POS(doc) for doc in docs]\n",
    "\n",
    "stop_words_lemmatizers = [preprocess_stop_words_lemmatizer(doc) for doc in seg_Token_POSs]\n",
    "processed_docs = [' '.join(doc) for doc in stop_words_lemmatizers]\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "df = pd.DataFrame(tfidf_matrix.T.toarray(), index=feature_names, columns=[f'Doc{i+1}' for i in range(len(docs))])\n",
    "posArr = np.empty(len(feature_names), dtype=f'<U{len(feature_names)}')\n",
    "for i in range(len(posArr)):\n",
    "    for poss in seg_Token_POSs:\n",
    "        for pos in poss:\n",
    "            for word, tag in pos:\n",
    "                if(word == feature_names[i]):\n",
    "                    posArr[i] = tag\n",
    "\n",
    "df['POS'] = posArr\n",
    "df.head()\n",
    "for i in range(len(docs)):\n",
    "    print(f\"\\nTop keywords in Document {i+1}:\")\n",
    "    top_keywords = df[f'Doc{i+1}'].sort_values(ascending=False).head(5)\n",
    "    print(top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c2147c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 TF-IDF Keywords:\n",
      "\n",
      "Sentence 1:\n",
      "text: 0.3536\n",
      "processing: 0.3536\n",
      "powerful: 0.3536\n",
      "natural: 0.3536\n",
      "important: 0.3536\n",
      "keywords: 0.3536\n",
      "language: 0.3536\n",
      "extract: 0.3536\n",
      "\n",
      "Sentence 2:\n",
      "word: 0.3162\n",
      "use: 0.3162\n",
      "technique: 0.3162\n",
      "significant: 0.3162\n",
      "nlp: 0.3162\n",
      "identify: 0.3162\n",
      "keyword: 0.3162\n",
      "help: 0.3162\n",
      "extraction: 0.3162\n",
      "common: 0.3162\n",
      "\n",
      "Sentence 3:\n",
      "tim: 0.2774\n",
      "opening: 0.2774\n",
      "new: 0.2774\n",
      "office: 0.2774\n",
      "open: 0.2774\n",
      "plan: 0.2774\n",
      "london: 0.2774\n",
      "cook: 0.2774\n",
      "apple: 0.2774\n",
      "attend: 0.2774\n",
      "ceremony: 0.2774\n",
      "july: 0.2774\n",
      "inc: 0.2774\n"
     ]
    }
   ],
   "source": [
    "print(\"🔑 TF-IDF Keywords:\")\n",
    "for i, sentence in enumerate(processed_docs):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    scores = tfidf_matrix[i].toarray()[0]\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    for idx in sorted_indices[:]: \n",
    "        if scores[idx] > 0:\n",
    "            print(f\"{feature_names[idx]}: {scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b15d0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏷 Named Entities:\n",
      "Natural: ADJ\n",
      "language: NOUN\n",
      "processing: NOUN\n",
      "with: ADP\n",
      "TF: PROPN\n",
      "-: PUNCT\n",
      "IDF: PROPN\n",
      "is: AUX\n",
      "powerful: ADJ\n",
      ".: PUNCT\n",
      "It: PRON\n",
      "extracts: VERB\n",
      "important: ADJ\n",
      "keywords: NOUN\n",
      "from: ADP\n",
      "text: NOUN\n",
      ".: PUNCT\n",
      "TF: PROPN\n",
      "-: PUNCT\n",
      "IDF: PROPN\n",
      "helps: VERB\n",
      "in: ADP\n",
      "identifying: VERB\n",
      "significant: ADJ\n",
      "words: NOUN\n",
      ".: PUNCT\n",
      "Keyword: PROPN\n",
      "extraction: NOUN\n",
      "using: VERB\n",
      "NLP: PROPN\n",
      "techniques: NOUN\n",
      "is: AUX\n",
      "common: ADJ\n",
      ".: PUNCT\n",
      "Keyword: PERSON\n",
      "NLP: ORG\n",
      "Apple: PROPN\n",
      "Inc.: PROPN\n",
      "is: AUX\n",
      "planning: VERB\n",
      "to: PART\n",
      "open: VERB\n",
      "a: DET\n",
      "new: ADJ\n",
      "office: NOUN\n",
      "in: ADP\n",
      "London: PROPN\n",
      ".: PUNCT\n",
      "Tim: PROPN\n",
      "Cook: PROPN\n",
      "will: AUX\n",
      "attend: VERB\n",
      "the: DET\n",
      "opening: NOUN\n",
      "ceremony: NOUN\n",
      "in: ADP\n",
      "July: PROPN\n",
      "2025: NUM\n",
      ".: PUNCT\n",
      "Apple Inc.: ORG\n",
      "London: GPE\n",
      "Tim Cook: PERSON\n",
      "July 2025: DATE\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🏷 Named Entities:\")\n",
    "named = [nlpNER(doc) for doc in docs]\n",
    "for elm in named:\n",
    "    for ent in elm:\n",
    "        print(f\"{ent.text}: {ent.pos_}\")\n",
    "    for ent in elm.ents:\n",
    "        print(f\"{ent.text}: {ent.label_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
